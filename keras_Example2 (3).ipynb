{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IXL9QaqLrXha",
    "outputId": "7c59bcca-fda1-4081-a31e-8a6ec0ed4429",
    "tags": []
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'maxnorm' from 'keras.constraints' (/Users/enochjoy/anaconda3/lib/python3.11/site-packages/keras/constraints/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dropout\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Flatten\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstraints\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m maxnorm\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SGD\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvolutional\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Conv2D\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'maxnorm' from 'keras.constraints' (/Users/enochjoy/anaconda3/lib/python3.11/site-packages/keras/constraints/__init__.py)"
     ]
    }
   ],
   "source": [
    "#Had to modify the code since I was getting errors on my computer\n",
    "\n",
    "\n",
    "# Simple CNN model for CIFAR-10\n",
    "import numpy\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "#from keras import backend as K\n",
    "#K.set_image_dim_ordering('th')\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "# normalize inputs from 0-255 to 0.0-1.0\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "# one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(32, 32, 3), padding='same', activation='relu', kernel_constraint=maxnorm(3)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same', kernel_constraint=maxnorm(3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu', kernel_constraint=maxnorm(3)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "# Compile model\n",
    "epochs = 5\n",
    "lrate = 0.01\n",
    "decay = lrate/epochs\n",
    "sgd = SGD(lr=lrate, momentum=0.9, decay=decay, nesterov=False)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=32)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 32, 32, 32)        896       \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 32, 32, 32)        0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 32, 32, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 16, 16, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 8192)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 512)               4194816   \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4210090 (16.06 MB)\n",
      "Trainable params: 4210090 (16.06 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 1.6717 - accuracy: 0.3941 - val_loss: 1.3948 - val_accuracy: 0.4950\n",
      "Epoch 2/5\n",
      "1563/1563 [==============================] - 38s 24ms/step - loss: 1.3367 - accuracy: 0.5175 - val_loss: 1.2319 - val_accuracy: 0.5656\n",
      "Epoch 3/5\n",
      "1563/1563 [==============================] - 39s 25ms/step - loss: 1.2129 - accuracy: 0.5693 - val_loss: 1.1504 - val_accuracy: 0.5913\n",
      "Epoch 4/5\n",
      "1563/1563 [==============================] - 39s 25ms/step - loss: 1.1348 - accuracy: 0.5958 - val_loss: 1.1097 - val_accuracy: 0.6100\n",
      "Epoch 5/5\n",
      "1563/1563 [==============================] - 41s 26ms/step - loss: 1.0762 - accuracy: 0.6196 - val_loss: 1.0736 - val_accuracy: 0.6224\n",
      "Accuracy: 62.24%\n"
     ]
    }
   ],
   "source": [
    "# Simple CNN model for CIFAR-10\n",
    "import numpy\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "from tensorflow.keras.optimizers import SGD  # Using the updated import path\n",
    "from tensorflow.keras.optimizers import legacy  # Importing the legacy optimizers\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# normalize inputs from 0-255 to 0.0-1.0\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# one hot encode outputs\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(32, 32, 3), padding='same', activation='relu', kernel_constraint=MaxNorm(3)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same', kernel_constraint=MaxNorm(3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu', kernel_constraint=MaxNorm(3)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile model\n",
    "epochs = 5\n",
    "lrate = 0.01\n",
    "decay = lrate / epochs\n",
    "# Use the legacy version of SGD as suggested by the warning\n",
    "sgd = legacy.SGD(learning_rate=lrate, momentum=0.9, decay=decay, nesterov=False)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "print(model.summary())\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=32)\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1] * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1562/1562 [==============================] - 142s 91ms/step - loss: 1.8248 - accuracy: 0.3532 - val_loss: 1.3980 - val_accuracy: 0.4791 - lr: 0.0010\n",
      "Epoch 2/25\n",
      "1562/1562 [==============================] - 154s 99ms/step - loss: 1.4374 - accuracy: 0.4804 - val_loss: 1.7104 - val_accuracy: 0.4803 - lr: 0.0010\n",
      "Epoch 3/25\n",
      "1562/1562 [==============================] - 150s 96ms/step - loss: 1.2938 - accuracy: 0.5421 - val_loss: 3.3843 - val_accuracy: 0.3870 - lr: 0.0010\n",
      "Epoch 4/25\n",
      "1562/1562 [==============================] - 153s 98ms/step - loss: 1.2038 - accuracy: 0.5761 - val_loss: 1.1907 - val_accuracy: 0.6005 - lr: 0.0010\n",
      "Epoch 5/25\n",
      "1562/1562 [==============================] - 155s 99ms/step - loss: 1.1339 - accuracy: 0.6002 - val_loss: 0.9587 - val_accuracy: 0.6656 - lr: 0.0010\n",
      "Epoch 6/25\n",
      "1562/1562 [==============================] - 156s 100ms/step - loss: 1.0973 - accuracy: 0.6158 - val_loss: 1.0638 - val_accuracy: 0.6475 - lr: 0.0010\n",
      "Epoch 7/25\n",
      "1562/1562 [==============================] - 156s 100ms/step - loss: 1.0620 - accuracy: 0.6270 - val_loss: 1.0673 - val_accuracy: 0.6478 - lr: 0.0010\n",
      "Epoch 8/25\n",
      "1562/1562 [==============================] - 155s 99ms/step - loss: 1.0453 - accuracy: 0.6301 - val_loss: 0.8595 - val_accuracy: 0.7037 - lr: 0.0010\n",
      "Epoch 9/25\n",
      "1562/1562 [==============================] - 155s 99ms/step - loss: 1.0066 - accuracy: 0.6450 - val_loss: 0.8921 - val_accuracy: 0.6904 - lr: 0.0010\n",
      "Epoch 10/25\n",
      "1562/1562 [==============================] - 156s 100ms/step - loss: 0.8809 - accuracy: 0.6915 - val_loss: 0.8502 - val_accuracy: 0.7166 - lr: 5.0000e-04\n",
      "Epoch 11/25\n",
      "1562/1562 [==============================] - 158s 101ms/step - loss: 0.8438 - accuracy: 0.7036 - val_loss: 0.7730 - val_accuracy: 0.7317 - lr: 5.0000e-04\n",
      "Epoch 12/25\n",
      "1562/1562 [==============================] - 159s 102ms/step - loss: 0.8298 - accuracy: 0.7098 - val_loss: 0.7826 - val_accuracy: 0.7379 - lr: 5.0000e-04\n",
      "Epoch 13/25\n",
      "1562/1562 [==============================] - 161s 103ms/step - loss: 0.8132 - accuracy: 0.7144 - val_loss: 0.6968 - val_accuracy: 0.7637 - lr: 5.0000e-04\n",
      "Epoch 14/25\n",
      "1562/1562 [==============================] - 158s 101ms/step - loss: 0.8057 - accuracy: 0.7160 - val_loss: 0.7157 - val_accuracy: 0.7560 - lr: 5.0000e-04\n",
      "Epoch 15/25\n",
      "1562/1562 [==============================] - 158s 101ms/step - loss: 0.8038 - accuracy: 0.7170 - val_loss: 0.7718 - val_accuracy: 0.7417 - lr: 5.0000e-04\n",
      "Epoch 16/25\n",
      "1562/1562 [==============================] - 160s 102ms/step - loss: 0.7966 - accuracy: 0.7214 - val_loss: 0.7243 - val_accuracy: 0.7547 - lr: 5.0000e-04\n",
      "Epoch 17/25\n",
      "1562/1562 [==============================] - 160s 102ms/step - loss: 0.7897 - accuracy: 0.7258 - val_loss: 0.6877 - val_accuracy: 0.7685 - lr: 5.0000e-04\n",
      "Epoch 18/25\n",
      "1562/1562 [==============================] - 175s 112ms/step - loss: 0.7849 - accuracy: 0.7251 - val_loss: 0.7289 - val_accuracy: 0.7620 - lr: 5.0000e-04\n",
      "Epoch 19/25\n",
      "1562/1562 [==============================] - 180s 115ms/step - loss: 0.7766 - accuracy: 0.7283 - val_loss: 0.7391 - val_accuracy: 0.7556 - lr: 5.0000e-04\n",
      "Epoch 20/25\n",
      "1562/1562 [==============================] - 180s 115ms/step - loss: 0.7188 - accuracy: 0.7478 - val_loss: 0.6752 - val_accuracy: 0.7728 - lr: 2.5000e-04\n",
      "Epoch 21/25\n",
      "1562/1562 [==============================] - 193s 124ms/step - loss: 0.6961 - accuracy: 0.7556 - val_loss: 0.6908 - val_accuracy: 0.7671 - lr: 2.5000e-04\n",
      "Epoch 22/25\n",
      "1562/1562 [==============================] - 192s 123ms/step - loss: 0.6774 - accuracy: 0.7637 - val_loss: 0.6342 - val_accuracy: 0.7852 - lr: 2.5000e-04\n",
      "Epoch 23/25\n",
      "1562/1562 [==============================] - 180s 115ms/step - loss: 0.6717 - accuracy: 0.7659 - val_loss: 0.6197 - val_accuracy: 0.7879 - lr: 2.5000e-04\n",
      "Epoch 24/25\n",
      "1562/1562 [==============================] - 181s 116ms/step - loss: 0.6636 - accuracy: 0.7699 - val_loss: 0.5785 - val_accuracy: 0.8010 - lr: 2.5000e-04\n",
      "Epoch 25/25\n",
      "1562/1562 [==============================] - 177s 114ms/step - loss: 0.6616 - accuracy: 0.7687 - val_loss: 0.5835 - val_accuracy: 0.8010 - lr: 2.5000e-04\n",
      "Accuracy: 80.10%\n"
     ]
    }
   ],
   "source": [
    "#1) With Adjusted Hyperparameters, Advanced Optimizers, Data Augmentation, and Regularization Techniques\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "from tensorflow.keras.optimizers.legacy import Adam  # Updated import path for legacy Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "# Define the model architecture with added layers, batch normalization, and adjusted dropout rates\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), input_shape=(32, 32, 3), padding='same', activation='relu', kernel_constraint=MaxNorm(3)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.2),\n",
    "    Conv2D(64, (3, 3), activation='relu', padding='same', kernel_constraint=MaxNorm(3)),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu', padding='same', kernel_constraint=MaxNorm(3)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu', kernel_constraint=MaxNorm(3)),\n",
    "    Dropout(0.5),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Learning rate scheduler function\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.001\n",
    "    drop = 0.5\n",
    "    epochs_drop = 10.0\n",
    "    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "    return lrate\n",
    "\n",
    "# Compile the model with the legacy Adam optimizer\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "\n",
    "# Data augmentation generator\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "datagen.fit(X_train)\n",
    "\n",
    "# Learning rate scheduler callback\n",
    "lrate = LearningRateScheduler(step_decay)\n",
    "callbacks_list = [lrate]\n",
    "\n",
    "# Train the model with data augmentation\n",
    "model.fit(datagen.flow(X_train, y_train, batch_size=32),\n",
    "          steps_per_epoch=X_train.shape[0] // 32, epochs=25,\n",
    "          validation_data=(X_test, y_test), callbacks=callbacks_list)\n",
    "\n",
    "# Evaluate the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Accuracy: %.2f%%' % (scores[1]*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 2. Provide logical description of which steps lead to improved response and what was its impact on \n",
    "#architecture behavior.\n",
    "\n",
    "#Improving the response of a convolutional neural network (CNN) on image classification tasks, like CIFAR-10, \n",
    "#involves a combination of tuning hyperparameters, adjusting the model architecture, and employing specific data \n",
    "#preprocessing techniques. The modifications introduced in the provided code aimed to enhance the model's ability \n",
    "#to generalize from the training data and perform more accurately on unseen test data.\n",
    "#Here's a logical description of the steps taken and their impact on the model's behavior:\n",
    "\n",
    "#1. Switching to Legacy Adam Optimizer\n",
    "#2. Adding Batch Normalization Layers\n",
    "#3. Adjusting Dropout Rates\n",
    "#4. Implementing Data Augmentation\n",
    "#5. Learning Rate Scheduling\n",
    "#6. Increasing Model Depth and Adjusting Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
